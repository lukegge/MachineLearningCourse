{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVegT5Vxjrl_"
      },
      "source": [
        "# Assignment 3 - Deep Learning\n",
        "\n",
        "Machine Learning (BBWL), Michael Mommert, FS2023, University of St. Gallen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgNs3wWyjrmC"
      },
      "source": [
        "The **goal** of this assignment is to implement and train a neural network to perform image classification. While a good performance of the resulting trained model is desirable, it is more important to follow the task setup carefully and implement your code following best practices.\n",
        "\n",
        "The dataset used is [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 32x32 RGB images, showing objects from either of 10 different classes. \n",
        "\n",
        "Your **objectives** are the following:\n",
        "* Implement a neural network architecture with at least 6 layers for the task of image classification. You can use any architecture you like.\n",
        "* For each training epoch, output the loss on the training dataset and the loss on the validation dataset. Tune the learning rate using this setup (only use full and half decimal powers, e.g., 0.001, 0.005, 0.01, 0.05, ...) to maximize the accuracy on the validation dataset and prevent overfitting. Visualize the training and validation loss as a function of epoch for the best-performing learning rate in the same plot.\n",
        "* Evaluate your final trained and tuned model on the test dataset by computing accuracy, precision and recall, visualize the confusion matrix and discuss implications.\n",
        "\n",
        "This assignment will be **graded** based on:\n",
        "* whether these objectives have been achieved;\n",
        "* whether the solution follows best practices;\n",
        "* how well the approach is documented (e.g., using text cells, plots, etc.);\n",
        "* how clean the code is.\n",
        "\n",
        "There are no restrictions on the resources that you can use -- collaborating on assignments is allowed -- but students are not allowed to submit identical code.\n",
        "\n",
        "There will be a leaderboard comparing the accuracies evaluated on the test dataset; the winner will receive a [grand prize](https://en.wikipedia.org/wiki/Mars_(chocolate_bar))!\n",
        "\n",
        "Please submit your runnable Notebook to [michael.mommert@unisg.ch](mailto:michael.mommert@unisg.ch) **before 17 May 2023, 23:59**. Please include your name in the Notebook filename.\n",
        "\n",
        "-----"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M0uTrLwNjrmE"
      },
      "source": [
        "## 1. Basic CNN Model\n",
        "Disclaimer: I ran the code locally and thus changed the first cell / imncluded it in the first basic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FB0ym7C-jrmH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LOG] notebook with cpu computation enabled\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1/10.. Training Loss: 2.303.. Validation Loss: 2.300\n",
            "Epoch: 2/10.. Training Loss: 2.293.. Validation Loss: 2.280\n",
            "Epoch: 3/10.. Training Loss: 2.233.. Validation Loss: 2.135\n",
            "Epoch: 4/10.. Training Loss: 2.069.. Validation Loss: 1.966\n",
            "Epoch: 5/10.. Training Loss: 1.950.. Validation Loss: 1.846\n",
            "Epoch: 6/10.. Training Loss: 1.862.. Validation Loss: 1.754\n",
            "Epoch: 7/10.. Training Loss: 1.784.. Validation Loss: 1.677\n",
            "Epoch: 8/10.. Training Loss: 1.724.. Validation Loss: 1.642\n",
            "Epoch: 9/10.. Training Loss: 1.680.. Validation Loss: 1.574\n",
            "Epoch: 10/10.. Training Loss: 1.650.. Validation Loss: 1.551\n",
            "Model evaluation:\n",
            "Accuracy: 0.4317\n",
            "F1-score: 0.41881326396450286\n",
            "Precision: 0.43054134206219496\n",
            "Recall: 0.4317\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "# Check for CUDA availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "print('[LOG] notebook with {} computation enabled'.format(str(device)))\n",
        "\n",
        "# Download and preprocess the CIFAR-10 dataset\n",
        "data_directory = './data_cifar10'\n",
        "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Just normalization for testing\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root=data_directory, train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=data_directory, train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred_batch = model(X_batch)\n",
        "            _, y_pred_batch = torch.max(y_pred_batch, dim=1)\n",
        "            y_pred.extend(y_pred_batch.tolist())\n",
        "            y_true.extend(y_batch.tolist())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Evaluation metric function\n",
        "def print_evaluation_scores(y_true, y_pred):\n",
        "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
        "    print('F1-score:', f1_score(y_true, y_pred, average='weighted'))\n",
        "    print('Precision:', precision_score(y_true, y_pred, average='weighted'))\n",
        "    print('Recall:', recall_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, criterion, optimizer)\n",
        "    valid_loss = evaluate(model, testloader, criterion)\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}.. Training Loss: {train_loss:.3f}.. Validation Loss: {valid_loss:.3f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_test_true, y_test_pred = predict(model, testloader)\n",
        "print(\"Model evaluation:\")\n",
        "print_evaluation_scores(y_test_true, y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkSHz53rUK0r"
      },
      "source": [
        "----"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Different Improvements\n",
        "### 2.1 Data augmentation\n",
        "Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model, by training models on several slightly-modified copies of existing data. (Wikipedia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1/10.. Training Loss: 1.622.. Validation Loss: 1.526\n",
            "Epoch: 2/10.. Training Loss: 1.594.. Validation Loss: 1.502\n",
            "Epoch: 3/10.. Training Loss: 1.575.. Validation Loss: 1.478\n",
            "Epoch: 4/10.. Training Loss: 1.552.. Validation Loss: 1.450\n",
            "Epoch: 5/10.. Training Loss: 1.536.. Validation Loss: 1.447\n",
            "Epoch: 6/10.. Training Loss: 1.516.. Validation Loss: 1.416\n",
            "Epoch: 7/10.. Training Loss: 1.501.. Validation Loss: 1.395\n",
            "Epoch: 8/10.. Training Loss: 1.482.. Validation Loss: 1.377\n",
            "Epoch: 9/10.. Training Loss: 1.471.. Validation Loss: 1.369\n",
            "Epoch: 10/10.. Training Loss: 1.458.. Validation Loss: 1.368\n",
            "Model evaluation:\n",
            "Accuracy: 0.5012\n",
            "F1-score: 0.4871487751660596\n",
            "Precision: 0.5021603326261208\n",
            "Recall: 0.5012\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "data_directory = './data_cifar10'\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_path = data_directory + '/train_cifar10'\n",
        "cifar10_train = torchvision.datasets.CIFAR10(root=train_path, train=True, download=True, transform=train_transform)\n",
        "train_loader = DataLoader(cifar10_train, batch_size=128, shuffle=True)\n",
        "\n",
        "eval_path = data_directory + '/eval_cifar10'\n",
        "cifar10_eval = torchvision.datasets.CIFAR10(root=eval_path, train=False, download=True, transform=val_test_transform)\n",
        "\n",
        "indices = list(range(len(cifar10_eval)))\n",
        "val_indices, test_indices = train_test_split(indices, test_size=0.5, stratify=cifar10_eval.targets, random_state=seed_value)\n",
        "\n",
        "val_dataset = Subset(cifar10_eval, val_indices)\n",
        "test_dataset = Subset(cifar10_eval, test_indices)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}.. Training Loss: {train_loss:.3f}.. Validation Loss: {valid_loss:.3f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_test_true, y_test_pred = predict(model, test_loader)\n",
        "print(\"Model evaluation:\")\n",
        "print_evaluation_scores(y_test_true, y_test_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Different Architecture\n",
        "We can use the ResNet-18 architecture with pre-trained weights from ImageNet, which is available in the torchvision.models module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lukas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lukas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\lukas/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:07<00:00, 6.46MB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 20\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model_resnet18, train_loader, criterion, optimizer)\n\u001b[0;32m     21\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model_resnet18, val_loader, criterion)\n\u001b[0;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m.. Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.. Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mvalid_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[8], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m     78\u001b[0m y_pred \u001b[39m=\u001b[39m model(X_batch)\n\u001b[0;32m     79\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[1;32m---> 80\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     81\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     82\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model_resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Adjust the last layer to match the number of CIFAR-10 classes\n",
        "num_classes = 10\n",
        "model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, num_classes)\n",
        "\n",
        "# Transfer the model to the appropriate device\n",
        "model_resnet18.to(device)\n",
        "\n",
        "# Set the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model_resnet18, train_loader, criterion, optimizer)\n",
        "    valid_loss = evaluate(model_resnet18, val_loader, criterion)\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}.. Training Loss: {train_loss:.3f}.. Validation Loss: {valid_loss:.3f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_test_true, y_test_pred = predict(model_resnet18, test_loader)\n",
        "print(\"Model evaluation:\")\n",
        "print_evaluation_scores(y_test_true, y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
