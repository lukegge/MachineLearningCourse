{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVegT5Vxjrl_"
      },
      "source": [
        "# Assignment 3 - Deep Learning\n",
        "\n",
        "Machine Learning (BBWL), Michael Mommert, FS2023, University of St. Gallen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgNs3wWyjrmC"
      },
      "source": [
        "The **goal** of this assignment is to implement and train a neural network to perform image classification. While a good performance of the resulting trained model is desirable, it is more important to follow the task setup carefully and implement your code following best practices.\n",
        "\n",
        "The dataset used is [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 32x32 RGB images, showing objects from either of 10 different classes. \n",
        "\n",
        "Your **objectives** are the following:\n",
        "* Implement a neural network architecture with at least 6 layers for the task of image classification. You can use any architecture you like.\n",
        "* For each training epoch, output the loss on the training dataset and the loss on the validation dataset. Tune the learning rate using this setup (only use full and half decimal powers, e.g., 0.001, 0.005, 0.01, 0.05, ...) to maximize the accuracy on the validation dataset and prevent overfitting. Visualize the training and validation loss as a function of epoch for the best-performing learning rate in the same plot.\n",
        "* Evaluate your final trained and tuned model on the test dataset by computing accuracy, precision and recall, visualize the confusion matrix and discuss implications.\n",
        "\n",
        "This assignment will be **graded** based on:\n",
        "* whether these objectives have been achieved;\n",
        "* whether the solution follows best practices;\n",
        "* how well the approach is documented (e.g., using text cells, plots, etc.);\n",
        "* how clean the code is.\n",
        "\n",
        "There are no restrictions on the resources that you can use -- collaborating on assignments is allowed -- but students are not allowed to submit identical code.\n",
        "\n",
        "There will be a leaderboard comparing the accuracies evaluated on the test dataset; the winner will receive a [grand prize](https://en.wikipedia.org/wiki/Mars_(chocolate_bar))!\n",
        "\n",
        "Please submit your runnable Notebook to [michael.mommert@unisg.ch](mailto:michael.mommert@unisg.ch) **before 17 May 2023, 23:59**. Please include your name in the Notebook filename.\n",
        "\n",
        "-----"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M0uTrLwNjrmE"
      },
      "source": [
        "The following code cells will setup the environment, download and prepare the data for you. Please do not modify these code cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FB0ym7C-jrmH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LOG] notebook with cpu computation enabled\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train: 50000, Val: 5000, Test: 5000\n"
          ]
        }
      ],
      "source": [
        "# import standard python libraries\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# import the PyTorch deep learning libary\n",
        "import torch, torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "# import sklearn classification evaluation library\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import plotting capabilities\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# init deterministic seed\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
        "\n",
        "# set cpu or gpu enabled device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "print('[LOG] notebook with {} computation enabled'.format(str(device)))\n",
        "\n",
        "# create data sub-directory in your local directory\n",
        "data_directory = './data_cifar10'\n",
        "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
        "\n",
        "# download training images and split data (X) from labels (y)\n",
        "train_path = data_directory + '/train_cifar10'\n",
        "cifar10_train = torchvision.datasets.CIFAR10(root=train_path, train=True, download=True)\n",
        "X_train = cifar10_train.data\n",
        "y_train = cifar10_train.targets\n",
        "\n",
        "# download evaluation images and split into val and test datasets\n",
        "eval_path = data_directory + '/eval_cifar10'\n",
        "cifar10_eval = torchvision.datasets.CIFAR10(root=eval_path, train=False, download=True)\n",
        "X_val, X_test, y_val, y_test = train_test_split(cifar10_eval.data, cifar10_eval.targets, test_size=0.5, stratify=cifar10_eval.targets, random_state=seed_value)\n",
        "\n",
        "# define class names\n",
        "cifar10_classes = cifar10_train.classes\n",
        "\n",
        "print('Train: {}, Val: {}, Test: {}'.format(len(X_train), len(X_val), len(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkSHz53rUK0r"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Akmbk8ubUK0s"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "def preprocess(data):\n",
        "    data = data.astype(np.float32) / 255.0\n",
        "    data = np.transpose(data, (0, 3, 1, 2))\n",
        "    return torch.from_numpy(data)\n",
        "\n",
        "X_train, X_val, X_test = preprocess(X_train), preprocess(X_val), preprocess(X_test)\n",
        "y_train, y_val, y_test = torch.tensor(y_train), torch.tensor(y_val), torch.tensor(y_test)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "\n",
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)\n",
        "\n",
        "\n",
        "# Train and evaluate the model\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, verbose=True):\n",
        "    train_losses, val_losses = [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        model.eval()\n",
        "        validation_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                \n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels)\n",
        "                validation_loss += loss.item()\n",
        "        \n",
        "        train_losses.append(running_loss/len(train_loader))\n",
        "        val_losses.append(validation_loss/len(val_loader))\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
        "                  \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "                  \"Validation Loss: {:.3f}.. \".format(val_losses[-1]))\n",
        "    \n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm as tqdm_auto\n",
        "\n",
        "# Update the train_model function to show a progress bar\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, verbose=True):\n",
        "    train_losses, val_losses = [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        \n",
        "        loop = tqdm_auto(train_loader, leave=False) if verbose else train_loader\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            if verbose:\n",
        "                loop.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
        "                loop.set_postfix(loss=loss.item())\n",
        "        \n",
        "        model.eval()\n",
        "        validation_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                \n",
        "                output = model(images)\n",
        "                loss = criterion(output, labels)\n",
        "                validation_loss += loss.item()\n",
        "        \n",
        "        train_losses.append(running_loss/len(train_loader))\n",
        "        val_losses.append(validation_loss/len(val_loader))\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
        "                  \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "                  \"Validation Loss: {:.3f}.. \".format(val_losses[-1]))\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Now re-run the model training and evaluation code blocks after this update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "# Tune learning rate\n",
        "learning_rates = [0.001, 0.005, 0.01, 0.05]\n",
        "best_val_loss = float('inf')\n",
        "best_lr = 0.001\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Trying learning rate: {lr}\")\n",
        "    net = Net().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    \n",
        "    train_losses, val_losses = train_model(net, criterion, optimizer, train_loader, val_loader, epochs=10, verbose=False)\n",
        "    if val_losses[-1] < best_val_loss:\n",
        "        best_val_loss = val_losses[-1]\n",
        "        best_lr = lr\n",
        "\n",
        "print(f\"Best learning rate: {best_lr}\")\n",
        "\n",
        "# Train the model with the best learning rate\n",
        "net = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=best_lr)\n",
        "train_losses, val_losses = train_model(net, criterion, optimizer, train_loader, val_loader, epochs=10)\n",
        "\n",
        "# Plot the training and validation losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "net.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = net(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_true, y_pred) * 100))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=cifar10_classes))\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
