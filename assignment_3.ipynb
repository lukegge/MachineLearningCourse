{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVegT5Vxjrl_"
      },
      "source": [
        "# Assignment 3 - Deep Learning\n",
        "\n",
        "Machine Learning (BBWL), Michael Mommert, FS2023, University of St. Gallen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgNs3wWyjrmC"
      },
      "source": [
        "The **goal** of this assignment is to implement and train a neural network to perform image classification. While a good performance of the resulting trained model is desirable, it is more important to follow the task setup carefully and implement your code following best practices.\n",
        "\n",
        "The dataset used is [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 32x32 RGB images, showing objects from either of 10 different classes. \n",
        "\n",
        "Your **objectives** are the following:\n",
        "* Implement a neural network architecture with at least 6 layers for the task of image classification. You can use any architecture you like.\n",
        "* For each training epoch, output the loss on the training dataset and the loss on the validation dataset. Tune the learning rate using this setup (only use full and half decimal powers, e.g., 0.001, 0.005, 0.01, 0.05, ...) to maximize the accuracy on the validation dataset and prevent overfitting. Visualize the training and validation loss as a function of epoch for the best-performing learning rate in the same plot.\n",
        "* Evaluate your final trained and tuned model on the test dataset by computing accuracy, precision and recall, visualize the confusion matrix and discuss implications.\n",
        "\n",
        "This assignment will be **graded** based on:\n",
        "* whether these objectives have been achieved;\n",
        "* whether the solution follows best practices;\n",
        "* how well the approach is documented (e.g., using text cells, plots, etc.);\n",
        "* how clean the code is.\n",
        "\n",
        "There are no restrictions on the resources that you can use -- collaborating on assignments is allowed -- but students are not allowed to submit identical code.\n",
        "\n",
        "There will be a leaderboard comparing the accuracies evaluated on the test dataset; the winner will receive a [grand prize](https://en.wikipedia.org/wiki/Mars_(chocolate_bar))!\n",
        "\n",
        "Please submit your runnable Notebook to [michael.mommert@unisg.ch](mailto:michael.mommert@unisg.ch) **before 17 May 2023, 23:59**. Please include your name in the Notebook filename.\n",
        "\n",
        "-----"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M0uTrLwNjrmE"
      },
      "source": [
        "## 1. Basic CNN Model\n",
        "Disclaimer: I ran the code locally and thus changed the first cell / imncluded it in the first basic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FB0ym7C-jrmH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LOG] notebook with cpu computation enabled\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1/10.. Training Loss: 2.303.. Validation Loss: 2.300\n",
            "Epoch: 2/10.. Training Loss: 2.293.. Validation Loss: 2.280\n",
            "Epoch: 3/10.. Training Loss: 2.233.. Validation Loss: 2.135\n",
            "Epoch: 4/10.. Training Loss: 2.069.. Validation Loss: 1.966\n",
            "Epoch: 5/10.. Training Loss: 1.950.. Validation Loss: 1.846\n",
            "Epoch: 6/10.. Training Loss: 1.862.. Validation Loss: 1.754\n",
            "Epoch: 7/10.. Training Loss: 1.784.. Validation Loss: 1.677\n",
            "Epoch: 8/10.. Training Loss: 1.724.. Validation Loss: 1.642\n",
            "Epoch: 9/10.. Training Loss: 1.680.. Validation Loss: 1.574\n",
            "Epoch: 10/10.. Training Loss: 1.650.. Validation Loss: 1.551\n",
            "Model evaluation:\n",
            "Accuracy: 0.4317\n",
            "F1-score: 0.41881326396450286\n",
            "Precision: 0.43054134206219496\n",
            "Recall: 0.4317\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "# Check for CUDA availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "print('[LOG] notebook with {} computation enabled'.format(str(device)))\n",
        "\n",
        "# Download and preprocess the CIFAR-10 dataset\n",
        "data_directory = './data_cifar10'\n",
        "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Just normalization for testing\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root=data_directory, train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=data_directory, train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move tensors to the device\n",
        "            y_pred_batch = model(X_batch)\n",
        "            _, y_pred_batch = torch.max(y_pred_batch, dim=1)\n",
        "            y_pred.extend(y_pred_batch.tolist())\n",
        "            y_true.extend(y_batch.tolist())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Evaluation metric function\n",
        "def print_evaluation_scores(y_true, y_pred):\n",
        "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
        "    print('F1-score:', f1_score(y_true, y_pred, average='weighted'))\n",
        "    print('Precision:', precision_score(y_true, y_pred, average='weighted'))\n",
        "    print('Recall:', recall_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, criterion, optimizer)\n",
        "    valid_loss = evaluate(model, testloader, criterion)\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}.. Training Loss: {train_loss:.3f}.. Validation Loss: {valid_loss:.3f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_test_true, y_test_pred = predict(model, testloader)\n",
        "print(\"Model evaluation:\")\n",
        "print_evaluation_scores(y_test_true, y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkSHz53rUK0r"
      },
      "source": [
        "----"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Different Improvements\n",
        "### 2.1 Data augmentation\n",
        "Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model, by training models on several slightly-modified copies of existing data. (Wikipedia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1/10.. Training Loss: 1.622.. Validation Loss: 1.526\n",
            "Epoch: 2/10.. Training Loss: 1.594.. Validation Loss: 1.502\n",
            "Epoch: 3/10.. Training Loss: 1.575.. Validation Loss: 1.478\n",
            "Epoch: 4/10.. Training Loss: 1.552.. Validation Loss: 1.450\n",
            "Epoch: 5/10.. Training Loss: 1.536.. Validation Loss: 1.447\n",
            "Epoch: 6/10.. Training Loss: 1.516.. Validation Loss: 1.416\n",
            "Epoch: 7/10.. Training Loss: 1.501.. Validation Loss: 1.395\n",
            "Epoch: 8/10.. Training Loss: 1.482.. Validation Loss: 1.377\n",
            "Epoch: 9/10.. Training Loss: 1.471.. Validation Loss: 1.369\n",
            "Epoch: 10/10.. Training Loss: 1.458.. Validation Loss: 1.368\n",
            "Model evaluation:\n",
            "Accuracy: 0.5012\n",
            "F1-score: 0.4871487751660596\n",
            "Precision: 0.5021603326261208\n",
            "Recall: 0.5012\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "data_directory = './data_cifar10'\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_path = data_directory + '/train_cifar10'\n",
        "cifar10_train = torchvision.datasets.CIFAR10(root=train_path, train=True, download=True, transform=train_transform)\n",
        "train_loader = DataLoader(cifar10_train, batch_size=128, shuffle=True)\n",
        "\n",
        "eval_path = data_directory + '/eval_cifar10'\n",
        "cifar10_eval = torchvision.datasets.CIFAR10(root=eval_path, train=False, download=True, transform=val_test_transform)\n",
        "\n",
        "indices = list(range(len(cifar10_eval)))\n",
        "val_indices, test_indices = train_test_split(indices, test_size=0.5, stratify=cifar10_eval.targets, random_state=seed_value)\n",
        "\n",
        "val_dataset = Subset(cifar10_eval, val_indices)\n",
        "test_dataset = Subset(cifar10_eval, test_indices)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}.. Training Loss: {train_loss:.3f}.. Validation Loss: {valid_loss:.3f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_test_true, y_test_pred = predict(model, test_loader)\n",
        "print(\"Model evaluation:\")\n",
        "print_evaluation_scores(y_test_true, y_test_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Different Architecture\n",
        "#### 2.2.1 ResNet-18\n",
        "We can use the ResNet-18 architecture with pre-trained weights from ImageNet, which is available in the torchvision.models module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10.. Training Loss: 1.044.. Validation Loss: 0.751\n",
            "Epoch: 2/10.. Training Loss: 0.745.. Validation Loss: 0.653\n",
            "Epoch: 3/10.. Training Loss: 0.655.. Validation Loss: 0.575\n",
            "Epoch: 4/10.. Training Loss: 0.598.. Validation Loss: 0.589\n",
            "Epoch: 5/10.. Training Loss: 0.566.. Validation Loss: 0.606\n",
            "Epoch: 6/10.. Training Loss: 0.526.. Validation Loss: 0.536\n",
            "Epoch: 7/10.. Training Loss: 0.490.. Validation Loss: 0.515\n",
            "Epoch: 8/10.. Training Loss: 0.474.. Validation Loss: 0.501\n",
            "Epoch: 9/10.. Training Loss: 0.448.. Validation Loss: 0.484\n",
            "Epoch: 10/10.. Training Loss: 0.432.. Validation Loss: 0.500\n",
            "Model evaluation:\n",
            "Accuracy: 0.827\n",
            "F1-score: 0.8276171032917266\n",
            "Precision: 0.8307913807903645\n",
            "Recall: 0.827\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model_resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Adjust the last layer to match the number of CIFAR-10 classes\n",
        "num_classes = 10\n",
        "model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, num_classes)\n",
        "\n",
        "# Transfer the model to the appropriate device\n",
        "model_resnet18.to(device)\n",
        "\n",
        "# Set the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model_resnet18, train_loader, criterion, optimizer)\n",
        "    valid_loss = evaluate(model_resnet18, val_loader, criterion)\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}.. Training Loss: {train_loss:.3f}.. Validation Loss: {valid_loss:.3f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_test_true, y_test_pred = predict(model_resnet18, test_loader)\n",
        "print(\"Model evaluation:\")\n",
        "print_evaluation_scores(y_test_true, y_test_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.2 μNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to C:\\Users\\lukas/.cache\\torch\\hub\\main.zip\n",
            "xFormers not available\n",
            "xFormers not available\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth\" to C:\\Users\\lukas/.cache\\torch\\hub\\checkpoints\\dinov2_vitg14_pretrain.pth\n",
            "100%|██████████| 4.23G/4.23G [04:22<00:00, 17.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
